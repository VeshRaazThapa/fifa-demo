{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasibzunair/residual-acgpn-demo/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CCSQIBMAYYZ"
      },
      "source": [
        "# Residual ACGPN Inference Pipeline\n",
        "\n",
        "This notebook generates a synthetic image of a person wearing a target clothing. It requires an image of a person and a target clothing as input.\n",
        "\n",
        "Built on top of https://github.com/hasibzunair/residual-acgpn-demo.\n",
        "\n",
        "Human parser and segmentation model from https://github.com/levindabhi/Self-Correction-Human-Parsing-for-ACGPN and https://github.com/levindabhi/U-2-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WJtP2PfBcPN"
      },
      "source": [
        "### Setup dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVm5QFBMDBbT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hasibzunair/residual-acgpn-demo.git\n",
        "%cd residual-acgpn-demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnk7syY0rPKp"
      },
      "outputs": [],
      "source": [
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQwI--uhoH6R"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import IPython\n",
        "import gdown\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from predict_pose import generate_pose_keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Ll7LIW1kGw"
      },
      "outputs": [],
      "source": [
        "# Create dummy folders same as train/test data dir tree\n",
        "\n",
        "!mkdir Data_preprocessing/test_color\n",
        "!mkdir Data_preprocessing/test_colormask\n",
        "!mkdir Data_preprocessing/test_edge\n",
        "!mkdir Data_preprocessing/test_img\n",
        "!mkdir Data_preprocessing/test_label\n",
        "!mkdir Data_preprocessing/test_mask\n",
        "!mkdir Data_preprocessing/test_pose\n",
        "!mkdir inputs\n",
        "!mkdir inputs/img\n",
        "!mkdir inputs/cloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8hYM6XqCnxC"
      },
      "outputs": [],
      "source": [
        "# Get pose model\n",
        "%cd pose\n",
        "!gdown --id 1hOHMFHEjhoJuLEQY0Ndurn5hfiA9mwko\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbGDB31KrKHV"
      },
      "outputs": [],
      "source": [
        "# Get parser and segmentation model\n",
        "\n",
        "!git clone https://github.com/levindabhi/Self-Correction-Human-Parsing-for-ACGPN.git\n",
        "!git clone https://github.com/levindabhi/U-2-Net.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hSJI347rZtQ"
      },
      "outputs": [],
      "source": [
        "# For segmentation mask generation\n",
        "url = 'https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH'\n",
        "output = 'lip_final.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzy8poZT6pcU"
      },
      "outputs": [],
      "source": [
        "%cd U-2-Net\n",
        "!mkdir saved_models\n",
        "!mkdir saved_models/u2net\n",
        "!mkdir saved_models/u2netp\n",
        "!gdown --id 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O saved_models/u2netp/u2netp.pth\n",
        "!gdown --id 1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ -O saved_models/u2net/u2net.pth\n",
        "import u2net_load\n",
        "import u2net_run\n",
        "u2net = u2net_load.model(model_name = 'u2netp')\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1VknOqswSTW"
      },
      "outputs": [],
      "source": [
        "# Get Residual ACGPN checkpoints\n",
        "\n",
        "!mkdir checkpoints\n",
        "%cd checkpoints\n",
        "!wget https://github.com/hasibzunair/residual-acgpn-demo/releases/download/v1.0/resacgpn.zip\n",
        "!unzip resacgpn.zip\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wKuYQvs53HH"
      },
      "outputs": [],
      "source": [
        "# For debugging!\n",
        "#!rm -rf checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD-DMczQthkd"
      },
      "source": [
        "### Please upload your cloth image below\n",
        "\n",
        "Some assumptions: \n",
        "* PNG image files\n",
        "* Name your cloth image as `cloth.png` and person image as `person.png`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAMmTiQ7zibP"
      },
      "outputs": [],
      "source": [
        "%cd inputs/cloth\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%cd ..\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQP04cGctrOW"
      },
      "source": [
        "### Please upload your person image below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wmUOR92170v"
      },
      "outputs": [],
      "source": [
        "%cd inputs/img\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%cd ..\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-zkuJWVWgrX"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm-9Up6z0Zpg"
      },
      "outputs": [],
      "source": [
        "cloth_name = 'cloth.png'\n",
        "cloth_path = os.path.join('inputs/cloth', sorted(os.listdir('inputs/cloth'))[0])\n",
        "cloth = Image.open(cloth_path)\n",
        "# Resize cloth image\n",
        "cloth = cloth.resize((192, 256), Image.BICUBIC).convert('RGB')\n",
        "\n",
        "# Remove hard-coding file names, under dev!\n",
        "#cloth_name = sorted(os.listdir('inputs/cloth'))[0]\n",
        "#cloth_name = cloth_name[:cloth_name.find('.')] + '.png'\n",
        "\n",
        "# Save resized cloth image\n",
        "cloth.save(os.path.join('Data_preprocessing/test_color', cloth_name))\n",
        "\n",
        "# 1. Get binary mask for clothing image\n",
        "u2net_run.infer(u2net, 'Data_preprocessing/test_color', 'Data_preprocessing/test_edge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3aHah45D655"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# Person image\n",
        "img_name = 'person.png'\n",
        "img_path = os.path.join('inputs/img', sorted(os.listdir('inputs/img'))[0])\n",
        "img = Image.open(img_path)\n",
        "img = img.resize((192,256), Image.BICUBIC)\n",
        "img_path = os.path.join('Data_preprocessing/test_img', img_name)\n",
        "img.save(img_path)\n",
        "resize_time = time.time()\n",
        "print('Resized image in {}s'.format(resize_time-start_time))\n",
        "\n",
        "# 2. Get parsed person image (test_label), uses person image\n",
        "!python3 Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py --dataset 'lip' --model-restore 'lip_final.pth' --input-dir 'Data_preprocessing/test_img' --output-dir 'Data_preprocessing/test_label'\n",
        "parse_time = time.time()\n",
        "print('Parsing generated in {}s'.format(parse_time-resize_time))\n",
        "\n",
        "# 3. Get pose map from person image\n",
        "pose_path = os.path.join('Data_preprocessing/test_pose', img_name.replace('.png', '_keypoints.json'))\n",
        "generate_pose_keypoints(img_path, pose_path)\n",
        "pose_time = time.time()\n",
        "print('Pose map generated in {}s'.format(pose_time-parse_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgMi912KAUNs"
      },
      "outputs": [],
      "source": [
        "!rm -rf Data_preprocessing/test_pairs.txt\n",
        "\n",
        "# Format: person, cloth image\n",
        "with open('Data_preprocessing/test_pairs.txt','w') as f:\n",
        "    f.write('person.png cloth.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTfjVq44X9dg"
      },
      "source": [
        "### Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIcoP4ll14Ia"
      },
      "outputs": [],
      "source": [
        "# Run test.py using the preferred configuration\n",
        "# We use resacgpn since we downloaded checkpoints for that config!\n",
        "\n",
        "!python test.py --name resacgpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IfZakpL1xX9"
      },
      "source": [
        "### Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11CogaUMJOpw"
      },
      "outputs": [],
      "source": [
        "# See 'results' directory\n",
        "output_grid = np.concatenate([np.array(Image.open('Data_preprocessing/test_img/person.png')),\n",
        "                np.array(Image.open('Data_preprocessing/test_color/cloth.png')),\n",
        "                np.array(Image.open('results/test/try-on/person.png'))], axis=1)\n",
        "image_grid = Image.fromarray(output_grid)\n",
        "image_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_zzoKdCOTNn"
      },
      "source": [
        "**To try a new person and cloth pair, remove current files and start again by uploading cloth image.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b-Fx2TyKbZ9"
      },
      "outputs": [],
      "source": [
        "!rm -rf Data_preprocessing/test_color/*\n",
        "!rm -rf Data_preprocessing/test_colormask/*\n",
        "!rm -rf Data_preprocessing/test_edge/*\n",
        "!rm -rf Data_preprocessing/test_img/*\n",
        "!rm -rf Data_preprocessing/test_label/*\n",
        "!rm -rf Data_preprocessing/test_mask/*\n",
        "!rm -rf Data_preprocessing/test_pose/*\n",
        "!rm -rf inputs/cloth/*\n",
        "!rm -rf inputs/img/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLbylWRlO9QW"
      },
      "outputs": [],
      "source": [
        "!rm -rf results/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNqv1z7JCSoQ"
      },
      "outputs": [],
      "source": [
        "%cd /content/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7WJtP2PfBcPN",
        "eD-DMczQthkd",
        "dQP04cGctrOW",
        "u-zkuJWVWgrX",
        "NTfjVq44X9dg",
        "0IfZakpL1xX9"
      ],
      "include_colab_link": true,
      "name": "demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
